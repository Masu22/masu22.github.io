# å¤–éƒ¨urlãƒªã‚¹ãƒˆã‹ã‚‰ã€å„urlã®htmlãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¦ã„ãï¼
# ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã€ãã®ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãã ã™ï¼

import requests
from bs4 import BeautifulSoup
import re

# ğŸ“Œ URLãƒªã‚¹ãƒˆã‚’å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
with open("urls.txt", "r", encoding="utf-8") as file:
    urls = [line.strip() for line in file.readlines() if line.strip()]

# ğŸ”§ ä¿å­˜ã§ããªã„æ–‡å­—ã‚’é™¤å¤–ã™ã‚‹é–¢æ•°
def clean_filename(title):
    return re.sub(r'[\\/*?:"<>|]', "", title)

# ğŸ“Œ å¤±æ•—ã—ãŸURLã‚’è¨˜éŒ²ã™ã‚‹ãƒªã‚¹ãƒˆ
failed_urls = []

# ğŸ¯ å„URLã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦HTMLãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼†ä¿å­˜
for i, url in enumerate(urls):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()

        # ğŸ› ï¸ HTMLã‚’è§£æ
        soup = BeautifulSoup(response.content, "html.parser")
        html_content = soup.prettify()

        # ğŸŒŸ ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã«ã™ã‚‹
        title = soup.title.string.strip() if soup.title else f"page_{i+1}"
        safe_title = clean_filename(title)

        # ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(f"{safe_title}.html", "w", encoding="utf-8") as file:
            file.write(html_content)

        print(f"âœ… {url} ã®ä¿å­˜æˆåŠŸï¼ â†’ {safe_title}.html")

    except Exception as e:
        # å¤±æ•—ã—ãŸURLã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        print(f"âš ï¸ {url} ã®ä¿å­˜å¤±æ•—: {e}")
        failed_urls.append(url)

# ğŸ“Œ å¤±æ•—ã—ãŸURLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
if failed_urls:
    with open("failed_urls.txt", "w", encoding="utf-8") as file:
        file.write("\n".join(failed_urls))
    print("âš ï¸ å¤±æ•—ã—ãŸURLã‚’ failed_urls.txt ã«ä¿å­˜ã—ã¾ã—ãŸï¼")

print("ğŸ‰ å®Œäº†ï¼")
